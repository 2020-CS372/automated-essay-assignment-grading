Figure 3. An Object-level Search Engine for Scientific Web
  Here P(w|O jk ) is the probability of generating w by the  th j  field of  record k.  P(w|O jk ) can be computed by treating each O jk  as a  document,  ( , ) ( , ) ( | ) (1 ) | | | | jk j jk jk j tf w O tf w C P w O O C λ λ = ⋅ + − ⋅   Where  j C  is the collection of all the  th j  fields of all the objects in  the object warehouse, and  λ is set according to Dirichlet prior  smoothing.   The intuition behind this formula is that we give different weight  to individual fields and give more weight to the correctly detected  and extracted records.   3.2.3 Model Balancing Record-level and Attribute- level Representations  As we discussed earlier, the unstructured object retrieval method  has the advantage of handling records with irregular patterns at the  expenses of ignoring the structure information, while attribute- level retrieval method can take the advantage of structure  information at the risk of amplifying extraction error.   We argue that the best way of scoring Web objects is to use the  accuracy of extracted object information as the parameter to find  the balance between structured and unstructured ways of scoring  the objects.  We use the formula below to estimate the probability  of generating term w by the language model of object o ,  1 1 1 ( | ) (1 ) ( | ) K M k k j k jk k j P w O P w O M α γ β γ = =     = + −         ∑ ∑   The basic intuition behind this formula is that we give different  weights to individual fields for correctly extracted records and  give the same weight to all the fields for the incorrectly extracted  records.   4. A Case Study   Below we will use Libra (http://libra.msra.cn), a working scientific  Web search engine we have built to motivate the need for object- level Web search and its advantages and challenges over existing  search engines.   As shown in Figure 3, we extract information from different Web  databases and pages to build structured databases of Web objects  including researchers, scientific papers, conferences, and journals.   The objects can be retrieved and ranked according to their  relevance to the query.  The relevance is calculated based on all the  collected information about this object, which is stored with  respect to each individual attribute.  For example, research paper  information is stored with respect to the following attributes: title,  author, year, conference, abstract, and full text.  In this way, we  can also handle structured queries and give different weights to  different attributes when calculating relevance scores.  Compared  WWW 2007 / Track: Data Mining Session: Identifying Structure in Web Pages  84with Google Scholar and CiteSeer, both of which solely search  paper information at the document level, this new engine can  retrieve and rank other types of Web objects.  This includes authors,  conferences and journals with respect to a query. 

